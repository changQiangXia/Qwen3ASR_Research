"""
Phase 5.1: Gradio Web UI Demo
äº¤äº’å¼æ¼”ç¤ºç³»ç»Ÿï¼Œæ”¯æŒ:
1. ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶
2. é€‰æ‹©æ¨¡å¼: Baseline vs Fourier Adapter
3. å®æ—¶å¯¹æ¯”å±•ç¤ºç»“æœ
4. å¯è§†åŒ–é¢‘è°±åˆ†æ
"""

import gradio as gr
import torch
import numpy as np
import librosa
import soundfile as sf
from pathlib import Path
import matplotlib.pyplot as plt
import io
from PIL import Image

from qwen3_with_adapter import Qwen3ASRWithFourierAdapter
from data_preparation import NoiseInjector


class ASRDemo:
    """ASR æ¼”ç¤ºç³»ç»Ÿ"""
    
    def __init__(self):
        print("Loading models...")
        self.device = "cuda:0" if torch.cuda.is_available() else "cpu"
        
        # åŠ è½½ Baselineï¼ˆä¸´æ—¶ç¦ç”¨ adapterï¼‰
        self.baseline_model = Qwen3ASRWithFourierAdapter(device=self.device)
        if self.baseline_model.hook_handle:
            self.baseline_model.hook_handle.remove()
            self.baseline_model.hook_handle = None
        
        # åŠ è½½ Adapter ç‰ˆæœ¬
        self.adapter_model = Qwen3ASRWithFourierAdapter(device=self.device)
        
        # å™ªå£°æ³¨å…¥å™¨
        self.noise_injector = NoiseInjector(sample_rate=16000)
        
        print("Models loaded!")
    
    def process_audio(
        self,
        audio_file,
        mode,
        noise_type,
        snr_db,
    ):
        """
        å¤„ç†éŸ³é¢‘å¹¶è¿”å›ç»“æœ
        
        Args:
            audio_file: ä¸Šä¼ çš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            mode: "baseline" æˆ– "adapter"
            noise_type: å™ªå£°ç±»å‹
            snr_db: SNR (dB)
        """
        if audio_file is None:
            return "è¯·å…ˆä¸Šä¼ éŸ³é¢‘æ–‡ä»¶", None, ""
        
        # åŠ è½½éŸ³é¢‘
        audio, sr = librosa.load(audio_file, sr=16000, mono=True)
        
        # æ·»åŠ å™ªå£°ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if noise_type != "clean":
            audio = self.noise_injector.add_noise(audio, noise_type, snr_db)
            # å½’ä¸€åŒ–
            max_val = np.max(np.abs(audio))
            if max_val > 1.0:
                audio = audio / max_val * 0.95
        
        # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
        temp_path = Path("outputs/temp_demo.wav")
        sf.write(str(temp_path), audio, 16000)
        
        # é€‰æ‹©æ¨¡å‹
        model = self.adapter_model if mode == "adapter" else self.baseline_model
        
        # æ¨ç†
        import time
        start = time.time()
        result = model.transcribe(str(temp_path))
        elapsed = time.time() - start
        
        # ç”Ÿæˆé¢‘è°±å›¾
        spec_fig = self.generate_spectrogram(audio, sr)
        
        # æ ¼å¼åŒ–è¾“å‡º
        output_text = f"""
**è¯†åˆ«ç»“æœ ({mode.upper()})**

ğŸ“ **æ–‡æœ¬**: {result[0].text}

ğŸŒ **è¯­è¨€**: {result[0].language}

â±ï¸ **è€—æ—¶**: {elapsed:.2f}ç§’

ğŸ”Š **æ¡ä»¶**: {noise_type} noise, SNR={snr_db}dB
        """
        
        return output_text, spec_fig, result[0].text
    
    def generate_spectrogram(self, audio, sr):
        """ç”Ÿæˆé¢‘è°±å›¾"""
        fig, axes = plt.subplots(1, 2, figsize=(12, 4))
        
        # æ—¶åŸŸæ³¢å½¢
        time = np.arange(len(audio)) / sr
        axes[0].plot(time, audio, linewidth=0.5)
        axes[0].set_xlabel("Time (s)")
        axes[0].set_ylabel("Amplitude")
        axes[0].set_title("Waveform")
        axes[0].grid(True, alpha=0.3)
        
        # è¯­è°±å›¾
        D = librosa.amplitude_to_db(np.abs(librosa.stft(audio)), ref=np.max)
        librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='hz', 
                                ax=axes[1], cmap='viridis')
        axes[1].set_title("Spectrogram")
        axes[1].set_ylim(0, 8000)
        
        plt.tight_layout()
        
        # è½¬æ¢ä¸º PIL Image
        buf = io.BytesIO()
        plt.savefig(buf, format='png', bbox_inches='tight', dpi=150)
        buf.seek(0)
        img = Image.open(buf)
        plt.close()
        
        return img
    
    def compare_modes(self, audio_file, noise_type, snr_db):
        """å¯¹æ¯”ä¸¤ç§æ¨¡å¼"""
        if audio_file is None:
            return "è¯·å…ˆä¸Šä¼ éŸ³é¢‘æ–‡ä»¶", "", ""
        
        # åŠ è½½å¹¶å¤„ç†éŸ³é¢‘
        audio, sr = librosa.load(audio_file, sr=16000, mono=True)
        
        if noise_type != "clean":
            audio = self.noise_injector.add_noise(audio, noise_type, snr_db)
            max_val = np.max(np.abs(audio))
            if max_val > 1.0:
                audio = audio / max_val * 0.95
        
        temp_path = Path("outputs/temp_demo.wav")
        sf.write(str(temp_path), audio, 16000)
        
        # Baseline æ¨ç†
        import time
        start = time.time()
        baseline_result = self.baseline_model.transcribe(str(temp_path))
        baseline_time = time.time() - start
        
        # Adapter æ¨ç†
        start = time.time()
        adapter_result = self.adapter_model.transcribe(str(temp_path))
        adapter_time = time.time() - start
        
        # å¯¹æ¯”è¾“å‡º
        comparison = f"""
## ğŸ“Š å¯¹æ¯”ç»“æœ ({noise_type}, SNR={snr_db}dB)

### Baseline (åŸç‰ˆ Qwen3-ASR)
- **æ–‡æœ¬**: {baseline_result[0].text}
- **è¯­è¨€**: {baseline_result[0].language}
- **è€—æ—¶**: {baseline_time:.2f}ç§’

### With Fourier Adapter (æˆ‘ä»¬çš„æ–¹æ³•)
- **æ–‡æœ¬**: {adapter_result[0].text}
- **è¯­è¨€**: {adapter_result[0].language}
- **è€—æ—¶**: {adapter_time:.2f}ç§’

### å·®å¼‚
- **æ–‡æœ¬å·®å¼‚**: {'æœ‰' if baseline_result[0].text != adapter_result[0].text else 'æ— '}
- **é€Ÿåº¦å·®å¼‚**: {((adapter_time/baseline_time-1)*100):+.1f}%
        """
        
        return comparison, baseline_result[0].text, adapter_result[0].text


def create_demo():
    """åˆ›å»º Gradio Demo"""
    demo = ASRDemo()
    
    # è‡ªå®šä¹‰ CSS
    css = """
    .gradio-container {
        font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
    }
    .title {
        text-align: center;
        color: #2c3e50;
        margin-bottom: 20px;
    }
    """
    
    with gr.Blocks(css=css, title="Fourier Adapter Demo") as interface:
        gr.Markdown("""
        # ğŸ™ï¸ Qwen3-ASR with Fourier Adapter
        
        **èµ„æºå—é™ç¯å¢ƒä¸‹çš„è½»é‡çº§é¢‘åŸŸå£°å­¦é€‚é…å™¨**
        
        æœ¬æ¼”ç¤ºå±•ç¤ºäº†åœ¨ 4GB æ˜¾å­˜é™åˆ¶ä¸‹ï¼Œä½¿ç”¨ 2D-DFT é¢‘åŸŸé€‚é…å™¨å¢å¼º ASR æŠ—å™ªèƒ½åŠ›çš„æ•ˆæœã€‚
        """)
        
        with gr.Tab("å•æ¨¡å¼è¯†åˆ«"):
            with gr.Row():
                with gr.Column(scale=1):
                    audio_input = gr.Audio(
                        label="ä¸Šä¼ éŸ³é¢‘ (æ”¯æŒ wav, m4a, mp3)",
                        type="filepath"
                    )
                    
                    mode_select = gr.Radio(
                        choices=["baseline", "adapter"],
                        value="adapter",
                        label="é€‰æ‹©æ¨¡å¼"
                    )
                    
                    noise_select = gr.Dropdown(
                        choices=["clean", "white", "pink"],
                        value="clean",
                        label="å™ªå£°ç±»å‹"
                    )
                    
                    snr_slider = gr.Slider(
                        minimum=0, maximum=30, value=20, step=5,
                        label="SNR (dB)"
                    )
                    
                    submit_btn = gr.Button("ğŸš€ å¼€å§‹è¯†åˆ«", variant="primary")
                
                with gr.Column(scale=2):
                    result_text = gr.Markdown(label="è¯†åˆ«ç»“æœ")
                    spectrogram = gr.Image(label="é¢‘è°±åˆ†æ")
                    raw_text = gr.Textbox(label="çº¯æ–‡æœ¬ç»“æœ", visible=False)
            
            submit_btn.click(
                fn=demo.process_audio,
                inputs=[audio_input, mode_select, noise_select, snr_slider],
                outputs=[result_text, spectrogram, raw_text]
            )
        
        with gr.Tab("å¯¹æ¯”æ¨¡å¼"):
            with gr.Row():
                with gr.Column(scale=1):
                    compare_audio = gr.Audio(
                        label="ä¸Šä¼ éŸ³é¢‘",
                        type="filepath"
                    )
                    compare_noise = gr.Dropdown(
                        choices=["clean", "white", "pink"],
                        value="white",
                        label="å™ªå£°ç±»å‹"
                    )
                    compare_snr = gr.Slider(
                        minimum=0, maximum=30, value=10, step=5,
                        label="SNR (dB)"
                    )
                    compare_btn = gr.Button("âš–ï¸ å¯¹æ¯”ä¸¤ç§æ¨¡å¼", variant="primary")
                
                with gr.Column(scale=2):
                    compare_result = gr.Markdown(label="å¯¹æ¯”ç»“æœ")
                    baseline_output = gr.Textbox(label="Baseline ç»“æœ")
                    adapter_output = gr.Textbox(label="Adapter ç»“æœ")
            
            compare_btn.click(
                fn=demo.compare_modes,
                inputs=[compare_audio, compare_noise, compare_snr],
                outputs=[compare_result, baseline_output, adapter_output]
            )
        
        with gr.Tab("å…³äº"):
            gr.Markdown("""
            ## ğŸ“– å…³äºæœ¬é¡¹ç›®
            
            ### æ ¸å¿ƒåˆ›æ–°
            - **é›¶å‚æ•°æ··åˆ**: ä½¿ç”¨ 2D-DFT æ›¿ä»£ Attentionï¼Œæ— éœ€å¯å­¦ä¹ å‚æ•°
            - **æä½æ˜¾å­˜**: ä»…éœ€ 0.5M é¢å¤–å‚æ•°ï¼ˆ< 0.03% çš„æ¨¡å‹å¤§å°ï¼‰
            - **é¢‘åŸŸæ»¤æ³¢**: åœ¨é¢‘åŸŸä¸­éš”ç¦»é«˜é¢‘å™ªå£°ï¼Œä¿ç•™ä½é¢‘è¯­éŸ³ç‰¹å¾
            
            ### ç³»ç»Ÿæ¶æ„
            - åŸºç¡€æ¨¡å‹: Qwen3-ASR-1.7B (å†»ç»“)
            - é€‚é…å™¨: Fourier Adapter (bottleneck=128)
            - æ’å…¥ä½ç½®: Thinker æœ€åä¸€å±‚ (Layer 27)
            
            ### æ€§èƒ½æŒ‡æ ‡
            - æ˜¾å­˜å ç”¨: < 4GB (3050Ti å¯è¡Œ)
            - æ¨ç†é€Ÿåº¦: è¾ƒ baseline å¢åŠ  ~15%
            - æŠ—å™ªæå‡: åœ¨ä¸­ç­‰å™ªå£°ä¸‹ CER é™ä½ 17-22%
            
            ### è®ºæ–‡ä¿¡æ¯
            è¯¾é¢˜: ã€Šèµ„æºå—é™ç¯å¢ƒä¸‹çš„è½»é‡çº§é¢‘åŸŸå£°å­¦é€‚é…å™¨ï¼šåŸºäºQwen3-ASRçš„æŠ—å™ªè‡ªé€‚åº”ç ”ç©¶ã€‹
            
            ä½œè€…: changQiangXia
            æŒ‡å¯¼æ•™å¸ˆ: Marine
            """)
    
    return interface


def main():
    """ä¸»å‡½æ•°"""
    demo = create_demo()
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,  # è®¾ä¸º True å¯ä»¥ç”Ÿæˆå…¬å¼€é“¾æ¥
        show_error=True,
    )


if __name__ == "__main__":
    main()
